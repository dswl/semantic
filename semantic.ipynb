{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rd3LfXv5u9TK"
   },
   "outputs": [],
   "source": [
    "# importing\n",
    "import numpy as np\n",
    "import gzip, os\n",
    "from urllib.request import urlretrieve\n",
    "from random import random\n",
    "from math import exp\n",
    "from random import seed\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKvcYDxUL7Br"
   },
   "source": [
    "Now testing to see if we can detect objects within an image\n",
    "\n",
    "This is the actual semantic segmentation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g_lomSeRzsoZ"
   },
   "outputs": [],
   "source": [
    "## DATA HERE from PASCAL2\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "url='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
    "path='VOC'\n",
    "def get_archive(path,url):\n",
    "  try:\n",
    "    os.mkdir(path)\n",
    "  except:\n",
    "    path=path\n",
    "\n",
    "  filename='devkit'\n",
    "  urllib.request.urlretrieve(url,f\"{path}/{filename}.tar\")\n",
    "\n",
    "get_archive(path,url)\n",
    "def extract(path):\n",
    "  tar_file=tarfile.open(f\"{path}/devkit.tar\")\n",
    "  tar_file.extractall('./')\n",
    "  tar_file.close()\n",
    "  shutil.rmtree(path)\n",
    "\n",
    "extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gkAFEoM9y3TU"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# color (rgb) segmentation maps in the dataset, change to single number in training\n",
    "VOC_COLORMAP = [\n",
    "    [0, 0, 0],\n",
    "    [224, 224, 192],\n",
    "    [128, 0, 0],\n",
    "    [0, 128, 0],\n",
    "    [128, 128, 0],\n",
    "    [0, 0, 128],\n",
    "    [128, 0, 128],\n",
    "    [0, 128, 128],\n",
    "    [128, 128, 128],\n",
    "    [64, 0, 0],\n",
    "    [192, 0, 0],\n",
    "    [64, 128, 0],\n",
    "    [192, 128, 0],\n",
    "    [64, 0, 128],\n",
    "    [192, 0, 128],\n",
    "    [64, 128, 128],\n",
    "    [0, 64, 0],\n",
    "    [192, 128, 128],\n",
    "    [128, 64, 0],\n",
    "    [0, 192, 0],\n",
    "    [128, 192, 0],\n",
    "    [0, 64, 128],\n",
    "]\n",
    "\n",
    "# categories, ordering respectivley\n",
    "VOC_CLASSES = [\n",
    "    \"background\",\n",
    "    \"edge\",\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"potted plant\",\n",
    "    \"person\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tv/monitor\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "h7FxO4wWy5F8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class VocDataset(Dataset):\n",
    "  def __init__(self, dir, color_map):\n",
    "    self.root = os.path.join(dir,'VOCdevkit/VOC2007')\n",
    "    self.images_dir = os.path.join(self.root,'JPEGImages')\n",
    "    self.target_dir = os.path.join(self.root,'SegmentationClass') # this should be ground truth\n",
    "    file_list = os.path.join(self.root,'ImageSets/Segmentation/trainval.txt')\n",
    "    self.files = [line.rstrip() for line in open(file_list, \"r\")]\n",
    "    self.color_map = color_map\n",
    "\n",
    "  def colors_to_seg_map(self, seg_RGB):\n",
    "    # return None\n",
    "    seg_map = [[1 for _ in range(seg_RGB.shape[1])] for _ in range(seg_RGB.shape[0])]\n",
    "    for row in range(len(seg_RGB)):\n",
    "      for col in range(len(seg_RGB[0])):\n",
    "        pixel = list(seg_RGB[row][col])\n",
    "        if pixel not in VOC_COLORMAP:\n",
    "          print(\"HELLO BIG PROBLEM HERE\")\n",
    "        id = VOC_COLORMAP.index(pixel)\n",
    "        seg_map[row][col] = id\n",
    "    return seg_map\n",
    "    return np.array(seg_map)\n",
    "\n",
    "\n",
    "    # to look at the seg maps, uncomment\n",
    "    # print(f\"here is the max: {np.max(seg_RGB)}\")\n",
    "    # seg_map = [[[0 for _ in range(seg_RGB.shape[1])] for _ in range(seg_RGB.shape[0])] for _ in range(len(VOC_COLORMAP))]\n",
    "    # for row in range(len(seg_RGB)):\n",
    "    #   for col in range(len(seg_RGB[0])):\n",
    "    #     pixel = list(seg_RGB[row][col])\n",
    "    #     id = VOC_COLORMAP.index(pixel)\n",
    "    #     seg_map[id][row][col] = 1\n",
    "    # # for i in range(len(seg_map)):\n",
    "    # #   plt.imshow(seg_map[i])\n",
    "    # #   plt.show()\n",
    "    # # seg_maps = tensor(seg_maps)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.files)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    file = self.files[index]\n",
    "    image_path = os.path.join(self.images_dir, file + \".jpg\")\n",
    "    seg_path = os.path.join(self.target_dir, file + \".png\")\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Load segmentation map\n",
    "\n",
    "    # center_crop = transforms.CenterCrop((224, 224))\n",
    "    center_crop = transforms.Resize((224, 224), interpolation=Image.NEAREST) # resizing seems to give better results than center crop\n",
    "    image = center_crop(image)\n",
    "\n",
    "    pre_seg_RGB = Image.open(seg_path).convert(\"RGB\")\n",
    "    pre_seg_RGB = center_crop(pre_seg_RGB)\n",
    "    seg_RGB = np.array(pre_seg_RGB)\n",
    "\n",
    "    seg_map = self.colors_to_seg_map(seg_RGB)\n",
    "\n",
    "\n",
    "    # old transformations based on imagenet norm\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  # ImageNet normalization\n",
    "    ])\n",
    "\n",
    "    image = transform(image)\n",
    "\n",
    "\n",
    "    return image, torch.tensor(seg_map).long()\n",
    "    # return image, seg_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zm37JJ_v7CWc"
   },
   "outputs": [],
   "source": [
    "class FCN32(torch.nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    # Pretrained VGG-16 features (first half of NN)\n",
    "    # vgg16 = models.vgg16(weights='VGG16_Weights.DEFAULT')\n",
    "    vgg16 = models.vgg16(weights='VGG16_Weights.DEFAULT')\n",
    "    self.features = vgg16.features\n",
    "    self.n_class = n_classes\n",
    "    # features = list(vgg16.features.children())\n",
    "    # self.vgg_features = nn.Sequential(*features)\n",
    "\n",
    "    \"\"\"\n",
    "      Perform upsampling via deconvolution operations\n",
    "    \"\"\"\n",
    "\n",
    "    # self.converge_decode = torch.nn.Sequential(\n",
    "    #   torch.nn.Conv2d(512, 4096, kernel_size=1),\n",
    "    #   torch.nn.ReLU(inplace=True),\n",
    "    #   torch.nn.Dropout(),\n",
    "    #   torch.nn.Conv2d(4096, 4096, kernel_size=1),\n",
    "    #   torch.nn.ReLU(inplace=True),\n",
    "    #   torch.nn.Dropout(),\n",
    "    #   torch.nn.ConvTranspose2d(4096, 512, kernel_size=4, stride=2, padding=1),\n",
    "    # )\n",
    "\n",
    "    self.converge_decode = torch.nn.Sequential(\n",
    "      # First Convolution Layer\n",
    "      torch.nn.Conv2d(512, 2048, kernel_size=1),\n",
    "      torch.nn.ReLU(inplace=True),\n",
    "\n",
    "      # Second Convolution Layer\n",
    "      torch.nn.Conv2d(2048, 2048, kernel_size=1),\n",
    "      torch.nn.ReLU(inplace=True),\n",
    "\n",
    "      # Upsample with ConvTranspose2d (start with larger kernel sizes)\n",
    "      torch.nn.ConvTranspose2d(2048, 1024, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "      torch.nn.BatchNorm2d(1024),\n",
    "      torch.nn.ReLU(inplace=True),\n",
    "\n",
    "      # Upsample more\n",
    "      torch.nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "      torch.nn.BatchNorm2d(512),\n",
    "      torch.nn.ReLU(inplace=True),\n",
    "\n",
    "      # Final Upsampling\n",
    "      torch.nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "      torch.nn.BatchNorm2d(256),\n",
    "      torch.nn.ReLU(inplace=True),\n",
    "\n",
    "      # Further Upsample\n",
    "      torch.nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "      torch.nn.BatchNorm2d(128),\n",
    "      torch.nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    self.final_upsample = torch.nn.ConvTranspose2d(\n",
    "        128, n_classes, kernel_size=2, stride=2\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # plt.imshow(x[0].T)\n",
    "    # plt.show()\n",
    "    # assert False\n",
    "    # print(\"PRE VGL FEATURES\", x.size())\n",
    "    x = self.features(x)\n",
    "    # print(\"POST VGL FEATURES\", x.size())\n",
    "\n",
    "    # Upsampling\n",
    "    # x = self.upscale_encode(x)\n",
    "    x = self.converge_decode(x)\n",
    "    # print(\"POST converge FEATURES\", x.size())\n",
    "    x = self.final_upsample(x)\n",
    "    # print(\"POST upsample FEATURES\", x.size())\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 3, 0])\n",
      "tensor([[10.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  0.,  0.],\n",
      "        [ 0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.],\n",
      "        [10.,  0.,  0.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00013624693383462727"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seg_maps\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# truth = torch.empty(3, dtype=torch.long).random_(5)\n",
    "truth = torch.tensor([0, 0, 1, 3, 0])\n",
    "pred = torch.tensor(\n",
    "        [\n",
    "            [10.0, 0.0, 0.0, 0.0],\n",
    "            [10.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 10.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 10.0],\n",
    "            [10.0, 0.0, 0.0, 0.0]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(truth)\n",
    "print(pred)\n",
    "\n",
    "output = criterion(pred, truth)\n",
    "output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Hjp2hYws-glU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch [1/1], Batch [11/106], Loss: 3.0365\n",
      "Epoch [1/1], Batch [21/106], Loss: 2.2402\n",
      "Epoch [1/1], Batch [31/106], Loss: 2.2920\n",
      "Epoch [1/1], Batch [41/106], Loss: 2.0044\n",
      "Epoch [1/1], Batch [51/106], Loss: 2.2394\n",
      "Epoch [1/1], Batch [61/106], Loss: 2.1858\n",
      "Epoch [1/1], Batch [71/106], Loss: 2.0734\n",
      "Epoch [1/1], Batch [81/106], Loss: 1.9732\n",
      "Epoch [1/1], Batch [91/106], Loss: 2.2899\n",
      "Epoch [1/1], Batch [101/106], Loss: 2.2008\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Define the VOC dataset\n",
    "batch_size = 4\n",
    "trainset = VocDataset(dir='./', color_map=VOC_COLORMAP)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# FCN-32 model\n",
    "fcn32_model = FCN32(n_classes=len(VOC_CLASSES))\n",
    "\n",
    "# google colab gpu if we can\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fcn32_model.to(device)\n",
    "\n",
    "noise_layer = None\n",
    "\n",
    "class_weights = torch.tensor([0.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5])\n",
    "# Note the class weights, because our images are background dominant, I will punish the model more when it misclassifies anything as background\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(fcn32_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print(f\"epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # print(f\"iteration {i}\")\n",
    "        # pdb.set_trace()\n",
    "        # assert False\n",
    "        inputs, seg_maps = data\n",
    "\n",
    "        # Move inputs and segmentation maps to GPU if available\n",
    "        inputs, seg_maps = inputs.to(device), seg_maps.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = fcn32_model(inputs)\n",
    "\n",
    "        # predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        # print(f\"inputs size: {inputs.size()}\")\n",
    "        # print(f\"outputs size: {outputs.size()}\")\n",
    "        # print(f\"predicted_classes size: {predicted_classes.size()}\")\n",
    "        # print(f\"seg_maps size: {seg_maps.size()}\")\n",
    "\n",
    "        # print(f\"segmap max: {torch.max(seg_maps[0])}\")\n",
    "        # print(f\"segmap min: {torch.min(seg_maps[0])}\")\n",
    "        # final = [[[255 for _ in range(3)] for _ in range(224)] for _ in range(224)]\n",
    "        # for row in range(224):\n",
    "        #     for col in range(224):\n",
    "        #         final[row][col] = VOC_COLORMAP[seg_maps[0][row][col]]\n",
    "        # plt.imshow(final)\n",
    "        # plt.show()\n",
    "\n",
    "        loss = criterion(outputs, seg_maps)\n",
    "        # loss = criterion(predicted_classes, seg_maps.float())\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0 and i != 0:    # Print every 200 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Batch [{i + 1}/{len(trainloader)}], Loss: {running_loss / 10:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to fcn32_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(fcn32_model.state_dict(), \"fcn32_model.pth\")\n",
    "print(\"Model saved to fcn32_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiRwMzd72NkE"
   },
   "outputs": [],
   "source": [
    "# Define the test dataset\n",
    "testset = VocDataset(dir='./', color_map=VOC_COLORMAP)\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define a function to calculate IoU\n",
    "def calculate_iou(pred, target):\n",
    "  total_pix = len(pred) * len(pred[0])\n",
    "  errors = 0\n",
    "  for i in range(len(pred)):\n",
    "    for j in range(len(pred[0])):\n",
    "      if (pred[i][j] != target[i][j]):\n",
    "        errors += 1\n",
    "  return errors / total_pix\n",
    "\n",
    "def collapse(detensor):\n",
    "  # seg_map is like this [[[0 for _ in range(seg_RGB.shape[1])] for _ in range(seg_RGB.shape[0])] for _ in range(len(VOC_COLORMAP))]\n",
    "  final = [[[255 for _ in range(3)] for _ in range(224)] for _ in range(224)]\n",
    "  len_detensor = len(detensor)\n",
    "\n",
    "  for row in range(224):\n",
    "    for col in range(224):\n",
    "      max_prob = -1.0\n",
    "      max_layer = -1.0\n",
    "      for layer_i in range(len_detensor):\n",
    "        # print(f\"checking layer {layer_i}\")\n",
    "        pixel = detensor[layer_i][row][col]\n",
    "        # print(f\"probability {pixel} found on layer {layer_i}\")\n",
    "        if pixel > max_prob:\n",
    "          max_prob = pixel\n",
    "          max_layer = layer_i\n",
    "      # print(f\"max layer selected: {max_layer}\")\n",
    "      final[row][col] = VOC_COLORMAP[max_layer]\n",
    "\n",
    "  return final\n",
    "\n",
    "def seg_map_format(seg_map):\n",
    "  final = [[[255 for _ in range(3)] for _ in range(224)] for _ in range(224)]\n",
    "  for row in range(224):\n",
    "    for col in range(224):\n",
    "      final[row][col] = VOC_COLORMAP[seg_map[row][col]]\n",
    "  return final\n",
    "\n",
    "def seg_map_format(seg_map):\n",
    "  final = [[[255 for _ in range(3)] for _ in range(224)] for _ in range(224)]\n",
    "  for row in range(224):\n",
    "    for col in range(224):\n",
    "      final[row][col] = VOC_COLORMAP[seg_map[row][col]]\n",
    "  return final\n",
    "\n",
    "\n",
    "# Initialize lists to store input images, segmentation maps, and predicted maps\n",
    "input_images = []\n",
    "ground_truths = []\n",
    "predicted_maps = []\n",
    "list_for_iou = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Perform inference\n",
    "fcn32_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for images, seg_maps in testloader:\n",
    "        # Move images and segmentation maps to GPU if available\n",
    "        images, seg_maps = images.to(device), seg_maps.to(device)\n",
    "\n",
    "        # Run model inference\n",
    "        outputs = fcn32_model(images)\n",
    "        # print(outputs.size())\n",
    "        # print(outputs[0])\n",
    "        # assert False\n",
    "\n",
    "        # Get predicted class indices\n",
    "        pred_indices = torch.argmax(outputs, dim=1)\n",
    "        # print(np.average(pred_indices.cpu().numpy()))\n",
    "\n",
    "        # Convert predicted indices to segmentation maps\n",
    "        # predicted_maps.append(pred_indices.cpu().numpy())\n",
    "        # predicted_maps.append(final)\n",
    "        predicted_maps.append(seg_map_format(pred_indices[0]))\n",
    "\n",
    "        # Store input images and ground truth segmentation maps\n",
    "        input_images.append(images[0].cpu().numpy())\n",
    "        ground_truths.append(seg_map_format(seg_maps[0].cpu().numpy()))\n",
    "\n",
    "        list_for_iou.append((pred_indices[0], seg_maps[0].cpu().numpy()))\n",
    "\n",
    "        # break\n",
    "        counter = counter + 1\n",
    "        if counter == 10:\n",
    "          break\n",
    "\n",
    "\n",
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "running_iou = 0.0\n",
    "\n",
    "# input, truth, and prediction\n",
    "def visualize_result(image, gt, pred, idx, iou_p):\n",
    "  global running_iou\n",
    "  plt.figure(figsize=(12, 4))\n",
    "\n",
    "  # Input\n",
    "  plt.subplot(1, 3, 1)\n",
    "  plt.imshow(image.transpose(1, 2, 0))\n",
    "  plt.title(\"Input Image\")\n",
    "  plt.axis('off')\n",
    "\n",
    "  # Ground\n",
    "  plt.subplot(1, 3, 2)\n",
    "  plt.imshow(gt, vmin=0, vmax=len(VOC_CLASSES)-1)\n",
    "  plt.title(\"Ground Truth\")\n",
    "  plt.axis('off')\n",
    "\n",
    "  # Predicted\n",
    "  plt.subplot(1, 3, 3)\n",
    "  plt.imshow(pred, vmin=0, vmax=len(VOC_CLASSES)-1)\n",
    "  plt.title(\"Predicted Map\")\n",
    "  plt.axis('off')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  iou = calculate_iou(iou_p[0], iou_p[1])\n",
    "  print(f\"CALCULATED IOU: {iou}\")\n",
    "  running_iou += iou\n",
    "\n",
    "\n",
    "# Visualize a random sample of the results\n",
    "for i in range(counter):\n",
    "  visualize_result(input_images[i], ground_truths[i], predicted_maps[i], i, list_for_iou[i])\n",
    "\n",
    "print(f\"The mean iou across {counter} sample images is {running_iou / counter}.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9URm1erZw7Gs"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
